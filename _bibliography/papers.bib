---
---

@string{CVPR = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),}}
@string{ECCV = {European Conference on Computer Vision (<b>ECCV</b>),}}
@string{IJCV = {International Journal of Computer Vision (<b>IJCV</b>),}}
@string{ICRA = {IEEE International Conference on Robotics and Automation (<b>ICRA</b>),}}
@string{arXiv = {arXiv preprint,}}


@inproceedings{kim2021wedge,
  title={WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation},
  author={Namyup Kim and Taeyoung Son and Jaehyun Pahk and Cuiling Lan and Wenjun Zeng and Suha Kwak},
  abstract={Domain generalization for semantic segmentation is highly demanded in real applications, where a trained model is expected to work well in previously unseen domains. One challenge lies in the lack of data which could cover the diverse distributions of the possible unseen domains for training. In this paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme, which is the first to exploit the diversity of web-crawled images for generalizable semantic segmentation. To explore and exploit the real-world data distributions, we collect a web-crawled dataset which presents large diversity in terms of weather conditions, sites, lighting, camera styles, etc. We also present a method which injects the style representation of the web-crawled data into the source domain on-the-fly during training, which enables the network to experience images of diverse styles with reliable labels for effective training. Moreover, we use the web-crawled dataset with predicted pseudo labels for training to further enhance the capability of the network. Extensive experiments demonstrate that our method clearly outperforms existing domain generalization techniques.},
  booktitle=ICRA,
  year={2023},
  abbr={ICRA},
  arxiv={2109.14196},
  selected={true},
  img_path={assets/img/publication_preview/kim2021wedge.jpg}
}

@inproceedings{lee2022fifo,
  title={FIFO: Learning Fog-Invariant Features for Foggy Scene Segmentation},
  author={Sohyun Lee and Taeyoung Son and Suha Kwak},
  abstract={Robust visual recognition under adverse weather conditions is of great importance in real-world applications. In this context, we propose a new method for learning semantic segmentation models robust against fog. Its key idea is to consider the fog condition of an image as its style and close the gap between images with different fog conditions in neural style spaces of a segmentation model. In particular, since the neural style of an image is in general affected by other factors as well as fog, we introduce a fog-pass filter module that learns to extract a fog-relevant factor from the style. Optimizing the fog-pass filter and the segmentation model alternately gradually closes the style gap between different fog conditions and allows to learn fog-invariant features in consequence. Our method substantially outperforms previous work on three real foggy image datasets. Moreover, it improves performance on both foggy and clear weather images, while existing methods often degrade performance on clear scenes.},
  booktitle=CVPR,
  year={2022},
  abbr={CVPR},
  arxiv={2204.01587},
  selected={true},
  code={https://github.com/sohyun-l/fifo},
  img_path={assets/img/publication_preview/lee2022fifo.jpg}
}


@inproceedings{son2020urie,
  title={Urie: Universal image enhancement for visual recognition in the wild},
  author={Taeyoung Son and Juwon Kang and Namyup Kim and Sunghyun Cho and Suha Kwak},
  abstract={Despite the great advances in visual recognition, it has been witnessed that recognition models trained on clean images of common datasets are not robust against distorted images in the real world. To tackle this issue, we present a Universal and Recognition-friendly Image Enhancement network, dubbed URIE, which is attached in front of existing recognition models and enhances distorted input to improve their performance without retraining them. URIE is universal in that it aims to handle various factors of image degradation and to be incorporated with any arbitrary recognition models. Also, it is recognition-friendly since it is optimized to improve the robustness of following recognition models, instead of perceptual quality of output image. Our experiments demonstrate that URIE can handle various and latent image distortions and improve the performance of existing models for five diverse recognition tasks when input images are degraded.},
  booktitle=ECCV,
  year={2020},
  abbr={ECCV},
  arxiv={2007.08979},
  selected={true},
  code={https://github.com/taeyoungson/urie},
  website={http://cvlab.postech.ac.kr/research/URIE/},
  img_path={assets/img/publication_preview/son2020urie.jpg}
}
